{
    "docs": [
        {
            "location": "/", 
            "text": "Installing AiC (local edition)\n\n\nThis repository contains the tools to build and run AiC in a \"single host, single user\" configuration.\n\n\nPrerequisites\n\n\nA modern linux distribution is required. It should be able to run Docker 1.10+, and qemu-kvm through libvirt.\n\n\nThe Docker images are built upon Ubuntu 16.04, and we assume the host is running Ubuntu 16.04 too, but it\nshould be easy to install on another distribution.\n\n\nTools\n\n\nAdd the docker repository\n\n\n$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n$ echo \ndeb https://apt.dockerproject.org/repo ubuntu-xenial main\n | sudo tee /etc/apt/sources.list.d/docker.list\n\n\n\n\nUnfortunately the apt package \"docker-compose\" is not compatible with\ndocker-engine, because it depends instead on the docker.io package.\nIt is therefore installed as a Python package instead. (if you prefer, you can\ninstall it in a virtualenv)\n\n\n$ sudo apt update\n$ sudo apt install -y git make python-pip docker-engine libvirt-bin qemu-kvm\n$ sudo pip install docker-compose\n\n\n\n\nThe commands in the next section assume that the current user can run Docker and libvirt.\n\n\nA simple way to allow that is to add the user to the docker and libvirtd groups:\n\n\n$ sudo adduser $(whoami) docker\n$ sudo adduser $(whoami) libvirtd\n\n\n\n\n(also logout and login again, to activate the new groups)\n\n\nThese groups might pose a security risk if you share the host with other\nprojects. Adding an untrusted user to the docker group is equivalent to granting\nhim root permissions, so you might prefer to manually prepend \"sudo\" to the\n\nmake\n commands and \n$AIC_HOME/bin/*\n scripts.\n\n\nInstallation overview\n\n\nThe following components will be deployed:\n\n\n\n\nA main directory ($AIC_HOME) to contain scripts, the Android VMs and system images\n\n\nThree Docker containers, shared among all VMs, to provide communication bus and API\n\n\nAn arbitrary number of Android x86 VMs, limited by the host's CPU and RAM.\n   You can change the default settings in lib/vm_template.xml\n\n\nA set of Docker containers for /each/ VM, to provide rendering and encoding,\n   device emulation and a NoVNC UI\n\n\n\n\n\n\nInstalling $AIC_HOME\n\n\nIf you haven't yet, choose the location of $AIC_HOME.\n\n\n$ export AIC_HOME=/path/to/local-aic\n$ git clone git@github.com:AiC-Project/local-aic.git $AIC_HOME\n$ cd $AIC_HOME\n\n\n\n\nInstall service and player images\n\n\nFrom binaries\n\n\n$ mkdir -p lib/images\n$ curl -Lo lib/images/services.tar https://github.com/AiC-Project/local-aic/releases/download/0.8/services.tar\n$ curl -Lo lib/images/player.tar https://github.com/AiC-Project/local-aic/releases/download/0.8/player.tar\n$ make docker-load\ndocker load -i lib/images/services.tar\nLoaded image: aic.senza:latest\ndocker load -i lib/images/player.tar\nLoaded image: aic.ffserver:latest\nLoaded image: aic.sensors:latest\nLoaded image: aic.xorg:latest\nLoaded image: aic.prjdata:latest\nLoaded image: aic.avmdata:latest\nLoaded image: aic.sdl:latest\nLoaded image: aic.camera:latest\nLoaded image: aic.audio:latest\n\n\n\n\nFrom sources\n\n\n$ make player-build services-build\ncd src; git clone git@github.com:AiC-Project/player.git\nCloning into 'player'...\nremote: Counting objects: 91, done.\n[...]\nRemoving intermediate container 10b63f273bd0\nSuccessfully built b15b7d2ef6ff\nTAG=dev docker-compose -f lib/docker/services/services.yml build\nrabbitmq uses an image, skipping\nsenza uses an image, skipping\n$\n\n\n\n\nInstall VM images\n\n\nFrom binaries\n\n\n$ curl -L https://github.com/AiC-Project/ats.rombuild/releases/download/0.8/aic-kitkat.tar | tar xf - -C lib/images/\n$ curl -L https://github.com/AiC-Project/ats.rombuild/releases/download/0.8/aic-lollipop.tar | tar xf - -C lib/images/\n\n\n\n\nFrom sources\n\n\nSee https://github.com/AiC-Project/ats.rombuild\n\n\nRunning AiC\n\n\nBackend services\n\n\nThe VMs need access to a few persistent services:\n\n\n\n\nAn AMQP server\n\n\nA REST API and CLI command to dispatch events\n\n\nA docker volume to contain APKs and video files\n\n\n\n\nThese were written with the multiuser version of AiC in mind, so they are running\nin a simplified configuration here.\nYou need to bring them up, as docker containers, and that's all. Note that rabbitmq\nis running with the default password (guest/guest).\n\n\n$ ./bin/services-up\nCreating network \nservices_default\n with the default driver\nCreating rabbitmq\nCreating senza\n$ ./bin/project-up\nWARNING: The AIC_PROJECT_PREFIX variable is not set. Defaulting to a blank string.\nCreating prjdata\n$\n\n\n\n\nCreating, listing, deleting VMs\n\n\nTo create a VM and the related player containers, run\n\n\n$ ./bin/vm-create path/to/image\n\n\n\n\nFor the image path, substitute the folder containing the version of your choice:\n\n\n\n\n\n\n\n\nVersion\n\n\nImage Path\n\n\n\n\n\n\n\n\n\n\n4.4.4 Phone\n\n\n./lib/images/android/aic-kitkat/gobyp\n\n\n\n\n\n\n4.4.4 Tablet\n\n\n./lib/images/android/aic-kitkat/gobyt\n\n\n\n\n\n\n5.1.1 Phone\n\n\n./lib/images/android/aic-lollipop/gobyp\n\n\n\n\n\n\n5.1.1 Tablet\n\n\n./lib/images/android/aic-lollipop/gobyt\n\n\n\n\n\n\n\n\nThe new VM will be assigned a random ID. You can list the VMs with the command\n\n\n$ ./bin/vm-list\nahFu1ief\nooHoh0ib\nWaeD8cei\nchohCh7f\nvum6h8ay\n\n\n\n\nVMs can be accessed with the Virtual Machine Manager, but the video output will not\nbe accessible since it must be decoded by a player service. See \"vm-browser\" below.\nThe Virtual Machine Manager displays the boot screen, and that's normal.\n\n\nRemoving a VM is done with\n\n\n$ ./bin/vm-delete vum6h8ay\nRemoving existing VM: vum6h8ay\n[...]\nNetwork services_default is external, skipping\nDomain vum6h8ay destroyed\n\nDomain vum6h8ay has been undefined\n\nVirtual machine destroyed.\n\n\n\n\nConnecting to a VM\n\n\nTo interact with the VM with a GUI, run\n\n\n$ ./bin/vm-browser kpbmvzpw\nOpening http://kanaka.github.io/noVNC/noVNC/vnc_auto.html?host=localhost\nport=32771\nCreated new window in existing browser session.\n\n\n\n\nYou should find a new browser tab where you can interact with the Android VM. Sound output\nis not enabled in the local version of AiC, though the audio stream is processed and available\nvia the ffserver container.\n\n\nTo open a shell on the VM, run\n\n\n$ ./bin/vm-adb kpbmvzpw shell\nroot@gobyp:/ #\n\n\n\n\nThe fb-adb script will forward any parameter to an \"adb\" process running in a dedicated container:\n\n\n$ ./bin/vm-adb kpbmvzpw shell getprop wifi.interface\neth1\n\n\n\n\nTransfering files\n\n\nThrough adb\n\n\nIf you need to transfer files with push/pull/install, the adb process will not have access to the host\nfilesystem, so it might be better to install the package android-tools-adb with apt.\nThe IP address of each VM is in ./libs/vms/{vm_id}/ip\n\n\n$ adb connect $(cat lib/vms/kpbmvzpw/ip)\nconnected to 192.168.122.214:5555\n$ adb -s $(cat lib/vms/kpbmvzpw/ip):5555 pull /init\n3776 KB/s (413355 bytes in 0.106s)\n\n\n\n\nInstalling an apk through Docker and vm-adb\n\n\nIf for some reason you have troubles using the regular adb commands, you can use docker to transfer files to the VM, through the aic.adb container.\n\n\n$ docker cp path/to/file.apk kpbmvzpw_adb:/tmp/\n$\n\n\n\n\nThen you can install an apk on the VM, using the vm-adb install command.\n\n\n./bin/vm-adb kpbmvzpw install /tmp/file.apk\n\n\n\n\nInteract with the VM sensors\n\n\nYou can emulate the state of the device's sensors, battery etc. by using the \"senza\"\ncommand.\n\n\n$ ./bin/senza help\nusage: senza [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]\n             [--config CONFIG] [--debug-requests]\n\nsenza\n\noptional arguments:\n  --version            show program's version number and exit\n  -v, --verbose        Increase verbosity of output. Can be repeated.\n  -q, --quiet          Suppress output except warnings and errors.\n  --log-file LOG_FILE  Specify a file to log output. Disabled by default.\n  -h, --help           Show help message and exit.\n  --debug              Show tracebacks on errors.\n  --config CONFIG      Configuration file (default: senza-client.ini)\n  --debug-requests     Print request details\n\nCommands:\n  accelerometer  Send a command to the accelerometer emulator\n  battery        Send a command to the battery emulator\n  camera         Select a video file to provide as camera input\n  complete       print bash completion command\n  gps            Send a command to the gps emulator\n  gravity        Send a command to the gravity sensor emulator\n  gsm call       Send a call command to the GSM emulator\n  gsm network    Send a command to the GSM emulator\n  gsm registration  Send a gsm registration command to the GSM emulator\n  gsm signal     Send a gsm network command to the GSM emulator\n  gsm sms        Send a SMS to the GSM emulator\n  gyroscope      Send a command to the gyroscope emulator\n  help           print detailed help for another command\n  light          Send a command to the light sensor emulator\n  linear-acc     Send a command to the linear_acc emulator\n  magnetometer   Send a command to the magnetometer emulator\n  orientation    Send a command to the orientation sensor emulator\n  pressure       Send a command to the pressure sensor emulator\n  proximity      Send a command to the proximity sensor emulator\n  recorder       Send a command to start or stop video recording\n  relative-humidity  Send a command to the relative_humidity sensor emulator\n  rotation-vector  Send a command to the rotation vector sensor emulator\n  schema         Display JSON schema for a given subcommand\n  temperature    Send a command to the thermometer emulator\n\n\n\n\nA detailed help is available for each subcommand:\n\n\n$ ./bin/senza help battery\nusage: senza battery [-h] avm_id level_percent ac_online status\n\nSend a command to the battery emulator\n\npositional arguments:\n  avm_id         AVM identifier\n  level_percent  battery level (0-100)\n  ac_online      0=battery; 1=AC\n  status         One of CHARGING, DISCHARGING, NOTCHARGING, FULL, UNKNOWN;\n                 default CHARGING\n\n\n\n\nCamera emulation\n\n\nThe emulated camera is initialized with an mpg file, which can be replaced once the VM is running.\n\n\nTo upload and activate a new video:\n\n\n$ ./bin/video-upload newcamera.mpg\n$ ./bin/senza camera 0y8ekugp newcamera.mpg\n\n\n\n\nThe upload is required only once (on the prjdata container) and activated separately for each VM.\nThe video must not contain an audio track. If the file is a still image, it will be\nconverted to mpg on the fly.\n\n\nChanging parameters\n\n\nThe VM resolution can be changed by setting the environment variables before running vm-create:\n\n\nAIC_PLAYER_WIDTH=800\nAIC_PLAYER_HEIGHT=600\n\n\n\n\nA too small resolution prevents the virtual machine from booting, and\na too big resolution can have performance issues.\n\n\nCommunication flow", 
            "title": "AiC Local"
        }, 
        {
            "location": "/#installing-aic-local-edition", 
            "text": "This repository contains the tools to build and run AiC in a \"single host, single user\" configuration.", 
            "title": "Installing AiC (local edition)"
        }, 
        {
            "location": "/#prerequisites", 
            "text": "A modern linux distribution is required. It should be able to run Docker 1.10+, and qemu-kvm through libvirt.  The Docker images are built upon Ubuntu 16.04, and we assume the host is running Ubuntu 16.04 too, but it\nshould be easy to install on another distribution.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/#tools", 
            "text": "Add the docker repository  $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n$ echo  deb https://apt.dockerproject.org/repo ubuntu-xenial main  | sudo tee /etc/apt/sources.list.d/docker.list  Unfortunately the apt package \"docker-compose\" is not compatible with\ndocker-engine, because it depends instead on the docker.io package.\nIt is therefore installed as a Python package instead. (if you prefer, you can\ninstall it in a virtualenv)  $ sudo apt update\n$ sudo apt install -y git make python-pip docker-engine libvirt-bin qemu-kvm\n$ sudo pip install docker-compose  The commands in the next section assume that the current user can run Docker and libvirt.  A simple way to allow that is to add the user to the docker and libvirtd groups:  $ sudo adduser $(whoami) docker\n$ sudo adduser $(whoami) libvirtd  (also logout and login again, to activate the new groups)  These groups might pose a security risk if you share the host with other\nprojects. Adding an untrusted user to the docker group is equivalent to granting\nhim root permissions, so you might prefer to manually prepend \"sudo\" to the make  commands and  $AIC_HOME/bin/*  scripts.", 
            "title": "Tools"
        }, 
        {
            "location": "/#installation-overview", 
            "text": "The following components will be deployed:   A main directory ($AIC_HOME) to contain scripts, the Android VMs and system images  Three Docker containers, shared among all VMs, to provide communication bus and API  An arbitrary number of Android x86 VMs, limited by the host's CPU and RAM.\n   You can change the default settings in lib/vm_template.xml  A set of Docker containers for /each/ VM, to provide rendering and encoding,\n   device emulation and a NoVNC UI", 
            "title": "Installation overview"
        }, 
        {
            "location": "/#installing-aic_home", 
            "text": "If you haven't yet, choose the location of $AIC_HOME.  $ export AIC_HOME=/path/to/local-aic\n$ git clone git@github.com:AiC-Project/local-aic.git $AIC_HOME\n$ cd $AIC_HOME", 
            "title": "Installing $AIC_HOME"
        }, 
        {
            "location": "/#install-service-and-player-images", 
            "text": "", 
            "title": "Install service and player images"
        }, 
        {
            "location": "/#from-binaries", 
            "text": "$ mkdir -p lib/images\n$ curl -Lo lib/images/services.tar https://github.com/AiC-Project/local-aic/releases/download/0.8/services.tar\n$ curl -Lo lib/images/player.tar https://github.com/AiC-Project/local-aic/releases/download/0.8/player.tar\n$ make docker-load\ndocker load -i lib/images/services.tar\nLoaded image: aic.senza:latest\ndocker load -i lib/images/player.tar\nLoaded image: aic.ffserver:latest\nLoaded image: aic.sensors:latest\nLoaded image: aic.xorg:latest\nLoaded image: aic.prjdata:latest\nLoaded image: aic.avmdata:latest\nLoaded image: aic.sdl:latest\nLoaded image: aic.camera:latest\nLoaded image: aic.audio:latest", 
            "title": "From binaries"
        }, 
        {
            "location": "/#from-sources", 
            "text": "$ make player-build services-build\ncd src; git clone git@github.com:AiC-Project/player.git\nCloning into 'player'...\nremote: Counting objects: 91, done.\n[...]\nRemoving intermediate container 10b63f273bd0\nSuccessfully built b15b7d2ef6ff\nTAG=dev docker-compose -f lib/docker/services/services.yml build\nrabbitmq uses an image, skipping\nsenza uses an image, skipping\n$", 
            "title": "From sources"
        }, 
        {
            "location": "/#install-vm-images", 
            "text": "", 
            "title": "Install VM images"
        }, 
        {
            "location": "/#from-binaries_1", 
            "text": "$ curl -L https://github.com/AiC-Project/ats.rombuild/releases/download/0.8/aic-kitkat.tar | tar xf - -C lib/images/\n$ curl -L https://github.com/AiC-Project/ats.rombuild/releases/download/0.8/aic-lollipop.tar | tar xf - -C lib/images/", 
            "title": "From binaries"
        }, 
        {
            "location": "/#from-sources_1", 
            "text": "See https://github.com/AiC-Project/ats.rombuild", 
            "title": "From sources"
        }, 
        {
            "location": "/#running-aic", 
            "text": "", 
            "title": "Running AiC"
        }, 
        {
            "location": "/#backend-services", 
            "text": "The VMs need access to a few persistent services:   An AMQP server  A REST API and CLI command to dispatch events  A docker volume to contain APKs and video files   These were written with the multiuser version of AiC in mind, so they are running\nin a simplified configuration here.\nYou need to bring them up, as docker containers, and that's all. Note that rabbitmq\nis running with the default password (guest/guest).  $ ./bin/services-up\nCreating network  services_default  with the default driver\nCreating rabbitmq\nCreating senza\n$ ./bin/project-up\nWARNING: The AIC_PROJECT_PREFIX variable is not set. Defaulting to a blank string.\nCreating prjdata\n$", 
            "title": "Backend services"
        }, 
        {
            "location": "/#creating-listing-deleting-vms", 
            "text": "To create a VM and the related player containers, run  $ ./bin/vm-create path/to/image  For the image path, substitute the folder containing the version of your choice:     Version  Image Path      4.4.4 Phone  ./lib/images/android/aic-kitkat/gobyp    4.4.4 Tablet  ./lib/images/android/aic-kitkat/gobyt    5.1.1 Phone  ./lib/images/android/aic-lollipop/gobyp    5.1.1 Tablet  ./lib/images/android/aic-lollipop/gobyt     The new VM will be assigned a random ID. You can list the VMs with the command  $ ./bin/vm-list\nahFu1ief\nooHoh0ib\nWaeD8cei\nchohCh7f\nvum6h8ay  VMs can be accessed with the Virtual Machine Manager, but the video output will not\nbe accessible since it must be decoded by a player service. See \"vm-browser\" below.\nThe Virtual Machine Manager displays the boot screen, and that's normal.  Removing a VM is done with  $ ./bin/vm-delete vum6h8ay\nRemoving existing VM: vum6h8ay\n[...]\nNetwork services_default is external, skipping\nDomain vum6h8ay destroyed\n\nDomain vum6h8ay has been undefined\n\nVirtual machine destroyed.", 
            "title": "Creating, listing, deleting VMs"
        }, 
        {
            "location": "/#connecting-to-a-vm", 
            "text": "To interact with the VM with a GUI, run  $ ./bin/vm-browser kpbmvzpw\nOpening http://kanaka.github.io/noVNC/noVNC/vnc_auto.html?host=localhost port=32771\nCreated new window in existing browser session.  You should find a new browser tab where you can interact with the Android VM. Sound output\nis not enabled in the local version of AiC, though the audio stream is processed and available\nvia the ffserver container.  To open a shell on the VM, run  $ ./bin/vm-adb kpbmvzpw shell\nroot@gobyp:/ #  The fb-adb script will forward any parameter to an \"adb\" process running in a dedicated container:  $ ./bin/vm-adb kpbmvzpw shell getprop wifi.interface\neth1", 
            "title": "Connecting to a VM"
        }, 
        {
            "location": "/#transfering-files", 
            "text": "", 
            "title": "Transfering files"
        }, 
        {
            "location": "/#through-adb", 
            "text": "If you need to transfer files with push/pull/install, the adb process will not have access to the host\nfilesystem, so it might be better to install the package android-tools-adb with apt.\nThe IP address of each VM is in ./libs/vms/{vm_id}/ip  $ adb connect $(cat lib/vms/kpbmvzpw/ip)\nconnected to 192.168.122.214:5555\n$ adb -s $(cat lib/vms/kpbmvzpw/ip):5555 pull /init\n3776 KB/s (413355 bytes in 0.106s)", 
            "title": "Through adb"
        }, 
        {
            "location": "/#installing-an-apk-through-docker-and-vm-adb", 
            "text": "If for some reason you have troubles using the regular adb commands, you can use docker to transfer files to the VM, through the aic.adb container.  $ docker cp path/to/file.apk kpbmvzpw_adb:/tmp/\n$  Then you can install an apk on the VM, using the vm-adb install command.  ./bin/vm-adb kpbmvzpw install /tmp/file.apk", 
            "title": "Installing an apk through Docker and vm-adb"
        }, 
        {
            "location": "/#interact-with-the-vm-sensors", 
            "text": "You can emulate the state of the device's sensors, battery etc. by using the \"senza\"\ncommand.  $ ./bin/senza help\nusage: senza [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]\n             [--config CONFIG] [--debug-requests]\n\nsenza\n\noptional arguments:\n  --version            show program's version number and exit\n  -v, --verbose        Increase verbosity of output. Can be repeated.\n  -q, --quiet          Suppress output except warnings and errors.\n  --log-file LOG_FILE  Specify a file to log output. Disabled by default.\n  -h, --help           Show help message and exit.\n  --debug              Show tracebacks on errors.\n  --config CONFIG      Configuration file (default: senza-client.ini)\n  --debug-requests     Print request details\n\nCommands:\n  accelerometer  Send a command to the accelerometer emulator\n  battery        Send a command to the battery emulator\n  camera         Select a video file to provide as camera input\n  complete       print bash completion command\n  gps            Send a command to the gps emulator\n  gravity        Send a command to the gravity sensor emulator\n  gsm call       Send a call command to the GSM emulator\n  gsm network    Send a command to the GSM emulator\n  gsm registration  Send a gsm registration command to the GSM emulator\n  gsm signal     Send a gsm network command to the GSM emulator\n  gsm sms        Send a SMS to the GSM emulator\n  gyroscope      Send a command to the gyroscope emulator\n  help           print detailed help for another command\n  light          Send a command to the light sensor emulator\n  linear-acc     Send a command to the linear_acc emulator\n  magnetometer   Send a command to the magnetometer emulator\n  orientation    Send a command to the orientation sensor emulator\n  pressure       Send a command to the pressure sensor emulator\n  proximity      Send a command to the proximity sensor emulator\n  recorder       Send a command to start or stop video recording\n  relative-humidity  Send a command to the relative_humidity sensor emulator\n  rotation-vector  Send a command to the rotation vector sensor emulator\n  schema         Display JSON schema for a given subcommand\n  temperature    Send a command to the thermometer emulator  A detailed help is available for each subcommand:  $ ./bin/senza help battery\nusage: senza battery [-h] avm_id level_percent ac_online status\n\nSend a command to the battery emulator\n\npositional arguments:\n  avm_id         AVM identifier\n  level_percent  battery level (0-100)\n  ac_online      0=battery; 1=AC\n  status         One of CHARGING, DISCHARGING, NOTCHARGING, FULL, UNKNOWN;\n                 default CHARGING", 
            "title": "Interact with the VM sensors"
        }, 
        {
            "location": "/#camera-emulation", 
            "text": "The emulated camera is initialized with an mpg file, which can be replaced once the VM is running.  To upload and activate a new video:  $ ./bin/video-upload newcamera.mpg\n$ ./bin/senza camera 0y8ekugp newcamera.mpg  The upload is required only once (on the prjdata container) and activated separately for each VM.\nThe video must not contain an audio track. If the file is a still image, it will be\nconverted to mpg on the fly.", 
            "title": "Camera emulation"
        }, 
        {
            "location": "/#changing-parameters", 
            "text": "The VM resolution can be changed by setting the environment variables before running vm-create:  AIC_PLAYER_WIDTH=800\nAIC_PLAYER_HEIGHT=600  A too small resolution prevents the virtual machine from booting, and\na too big resolution can have performance issues.", 
            "title": "Changing parameters"
        }, 
        {
            "location": "/#communication-flow", 
            "text": "", 
            "title": "Communication flow"
        }, 
        {
            "location": "/gpu/README.gpu/", 
            "text": "Using the GPU for the rendering\n\n\nThe official docker images use\n\nmesa\u2019s llvmpipe renderer\n in order\nto have a fully isolated OpenGL rendering process. This allows AiC to ignore\nsystem specificities and run without issues anywhere. However, its performance\nis quite limited even on powerful CPUs and it is therefore useful to use a\ngraphics card (even an intel integrated one) to avoid that bottleneck, which\ngets worse the more AiC virtual machines you are running.\n\n\nTo use the host graphics card while still displaying content in a\ncontainerized X server, we use \nVirtualGL\n\nand the \nvglrun\n tool it includes.\n\n\nTo this end, the \"sdl\" container needs to share the X11 socket (which is\nusually in \n/tmp/.X11-unix\n) from the host, and the linux rendering device\n(see the nvidia compose file at the bottom of this page as an example).\n\n\nThe scripts also have to be changed in order to run the software through\nvglrun, which is done in the start_sdl script as an example.\nUsually it is enough to prefix the commands with (where \n:0\n is the host\nX server to use for rendering)\n\n\nvglrun -c 0 -d :0\n\n\n\n\nPlease note that this could be a security breach if an attacker takes control\nof your AiC virtual device and finds a way to control the \"sdl\" container, as X\nsecurity and isolation is notorious for not existing.\n\n\nMesa\n\n\nDockerfile\n\n\nYou need to add the relevant mesa drivers to the list of packages to\ninstall in the container image. This is setup-specific so we will\nnot cover it here.\n\n\nYou also need to install virtualgl, which is not in the official ubuntu\nrepos, so you need to download the \nlatest deb\n\nand install it inside the Dockerfile.\n\n\nCOPY virtualgl_2.X_amd64.deb /tmp/virtualgl_2.X_amd64.deb\nRUN dpkg -i /tmp/virtualgl_2.X_amd64.deb\n\n\n\n\nDocker-compose\n\n\nYou will need to change the docker-compose file in order to share the\nX server socket from the host in a read/write volume to the \"sdl\" container:\n\n\n  sdl:\n    \u2026\n    volumes:\n      \u2026\n      - /tmp/.X11-unix/:/tmp/.X11-unix/:rw\n\n\n\n\nAnd you will need to share /dev/dri/card0 (or not card0, depending on\nyour setup):\n\n\nsdl:\n    \u2026\n    devices:\n      - /dev/dri/card0\n\n\n\n\nNvidia\n\n\nDockerfile\n\n\nYou need to start from the\n\nnvidia-docker opengl Dockerfile\n,\ntagged opengl:virtualgl (but based on ubuntu 16.04, not 14.04), then\ncopy the sdl.Dockerfile and the start_sdl into the src/player directory,\nand do a source build to recreate the container images.\n\n\ndocker-compose\n\n\nThe easiest way is to install the\n\nnvidia-docker plugin\n\n(needs a docker restart) and replace the run-player.yml file in\nlib/docker/ with the following:\n\n\nversion: \n2\n\n\nservices:\n  xorg:\n    container_name: \n${AIC_AVM_PREFIX}xorg\n\n    restart: unless-stopped\n    image: aic.xorg\n    ports:\n      - 5900\n    environment:\n      - AIC_PLAYER_VNC_SECRET\n      - AIC_PLAYER_MAX_DIMENSION\n  ffserver:\n    container_name: \n${AIC_AVM_PREFIX}ffserver\n\n    restart: unless-stopped\n    image: aic.ffserver\n    ports:\n      - 8090\n  adb:\n    container_name: \n${AIC_AVM_PREFIX}adb\n\n    restart: unless-stopped\n    image: aic.adb\n    volumes_from:\n      - container:${AIC_PROJECT_PREFIX}prjdata:ro\n    environment:\n      - AIC_PLAYER_VM_HOST\n  sdl:\n    container_name: \n${AIC_AVM_PREFIX}sdl\n\n    restart: unless-stopped\n    devices:\n      - /dev/nvidia0\n      - /dev/nvidiactl\n      - /dev/nvidia-uvm\n    image: aic.sdl\n    volumes:\n      - /tmp/.X11-unix:/tmp/.X11-unix:rw\n      - nvidia_driver_370.28:/usr/local/nvidia:ro\n    volumes_from:\n      - avmdata\n    # command: tail -f /dev/null\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_ID\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_WIDTH\n      - AIC_PLAYER_HEIGHT\n      - AIC_PLAYER_DPI\n      - AIC_PLAYER_ENABLE_RECORD\n      - AIC_PLAYER_ANDROID_VERSION\n      - AIC_PLAYER_PATH_RECORD\n    # to be able to strace\n    privileged: false\n    depends_on:\n      - xorg\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  audio:\n    # command: tail -f /dev/null\n    container_name: \n${AIC_AVM_PREFIX}audio\n\n    restart: unless-stopped\n    image: aic.audio\n    environment:\n      - AIC_PLAYER_VM_HOST\n    depends_on:\n      - ffserver\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  sensors:\n    # command: tail -f /dev/null\n    container_name: \n${AIC_AVM_PREFIX}sensors\n\n    restart: unless-stopped\n    image: aic.sensors\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_VM_ID\n      - AIC_PLAYER_ENABLE_SENSORS\n      - AIC_PLAYER_ENABLE_BATTERY\n      - AIC_PLAYER_ENABLE_GPS\n      - AIC_PLAYER_ENABLE_GSM\n      - AIC_PLAYER_ENABLE_NFC\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  camera:\n    # command: tail -f /dev/null\n    container_name: \n${AIC_AVM_PREFIX}camera\n\n    restart: unless-stopped\n    image: aic.camera\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_VM_ID\n    volumes_from:\n      - container:${AIC_PROJECT_PREFIX}prjdata:ro\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  avmdata:\n    container_name: \n${AIC_AVM_PREFIX}avmdata\n\n    restart: unless-stopped\n    image: aic.avmdata\n    volumes:\n      - /data/avm\n    networks: []\n\nnetworks:\n  services_default:\n    external: true\n  default:\n\nvolumes:\n  nvidia_driver_370.28:\n    external: true", 
            "title": "GPU"
        }, 
        {
            "location": "/gpu/README.gpu/#using-the-gpu-for-the-rendering", 
            "text": "The official docker images use mesa\u2019s llvmpipe renderer  in order\nto have a fully isolated OpenGL rendering process. This allows AiC to ignore\nsystem specificities and run without issues anywhere. However, its performance\nis quite limited even on powerful CPUs and it is therefore useful to use a\ngraphics card (even an intel integrated one) to avoid that bottleneck, which\ngets worse the more AiC virtual machines you are running.  To use the host graphics card while still displaying content in a\ncontainerized X server, we use  VirtualGL \nand the  vglrun  tool it includes.  To this end, the \"sdl\" container needs to share the X11 socket (which is\nusually in  /tmp/.X11-unix ) from the host, and the linux rendering device\n(see the nvidia compose file at the bottom of this page as an example).  The scripts also have to be changed in order to run the software through\nvglrun, which is done in the start_sdl script as an example.\nUsually it is enough to prefix the commands with (where  :0  is the host\nX server to use for rendering)  vglrun -c 0 -d :0  Please note that this could be a security breach if an attacker takes control\nof your AiC virtual device and finds a way to control the \"sdl\" container, as X\nsecurity and isolation is notorious for not existing.", 
            "title": "Using the GPU for the rendering"
        }, 
        {
            "location": "/gpu/README.gpu/#mesa", 
            "text": "", 
            "title": "Mesa"
        }, 
        {
            "location": "/gpu/README.gpu/#dockerfile", 
            "text": "You need to add the relevant mesa drivers to the list of packages to\ninstall in the container image. This is setup-specific so we will\nnot cover it here.  You also need to install virtualgl, which is not in the official ubuntu\nrepos, so you need to download the  latest deb \nand install it inside the Dockerfile.  COPY virtualgl_2.X_amd64.deb /tmp/virtualgl_2.X_amd64.deb\nRUN dpkg -i /tmp/virtualgl_2.X_amd64.deb", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/gpu/README.gpu/#docker-compose", 
            "text": "You will need to change the docker-compose file in order to share the\nX server socket from the host in a read/write volume to the \"sdl\" container:    sdl:\n    \u2026\n    volumes:\n      \u2026\n      - /tmp/.X11-unix/:/tmp/.X11-unix/:rw  And you will need to share /dev/dri/card0 (or not card0, depending on\nyour setup):  sdl:\n    \u2026\n    devices:\n      - /dev/dri/card0", 
            "title": "Docker-compose"
        }, 
        {
            "location": "/gpu/README.gpu/#nvidia", 
            "text": "", 
            "title": "Nvidia"
        }, 
        {
            "location": "/gpu/README.gpu/#dockerfile_1", 
            "text": "You need to start from the nvidia-docker opengl Dockerfile ,\ntagged opengl:virtualgl (but based on ubuntu 16.04, not 14.04), then\ncopy the sdl.Dockerfile and the start_sdl into the src/player directory,\nand do a source build to recreate the container images.", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/gpu/README.gpu/#docker-compose_1", 
            "text": "The easiest way is to install the nvidia-docker plugin \n(needs a docker restart) and replace the run-player.yml file in\nlib/docker/ with the following:  version:  2 \n\nservices:\n  xorg:\n    container_name:  ${AIC_AVM_PREFIX}xorg \n    restart: unless-stopped\n    image: aic.xorg\n    ports:\n      - 5900\n    environment:\n      - AIC_PLAYER_VNC_SECRET\n      - AIC_PLAYER_MAX_DIMENSION\n  ffserver:\n    container_name:  ${AIC_AVM_PREFIX}ffserver \n    restart: unless-stopped\n    image: aic.ffserver\n    ports:\n      - 8090\n  adb:\n    container_name:  ${AIC_AVM_PREFIX}adb \n    restart: unless-stopped\n    image: aic.adb\n    volumes_from:\n      - container:${AIC_PROJECT_PREFIX}prjdata:ro\n    environment:\n      - AIC_PLAYER_VM_HOST\n  sdl:\n    container_name:  ${AIC_AVM_PREFIX}sdl \n    restart: unless-stopped\n    devices:\n      - /dev/nvidia0\n      - /dev/nvidiactl\n      - /dev/nvidia-uvm\n    image: aic.sdl\n    volumes:\n      - /tmp/.X11-unix:/tmp/.X11-unix:rw\n      - nvidia_driver_370.28:/usr/local/nvidia:ro\n    volumes_from:\n      - avmdata\n    # command: tail -f /dev/null\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_ID\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_WIDTH\n      - AIC_PLAYER_HEIGHT\n      - AIC_PLAYER_DPI\n      - AIC_PLAYER_ENABLE_RECORD\n      - AIC_PLAYER_ANDROID_VERSION\n      - AIC_PLAYER_PATH_RECORD\n    # to be able to strace\n    privileged: false\n    depends_on:\n      - xorg\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  audio:\n    # command: tail -f /dev/null\n    container_name:  ${AIC_AVM_PREFIX}audio \n    restart: unless-stopped\n    image: aic.audio\n    environment:\n      - AIC_PLAYER_VM_HOST\n    depends_on:\n      - ffserver\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  sensors:\n    # command: tail -f /dev/null\n    container_name:  ${AIC_AVM_PREFIX}sensors \n    restart: unless-stopped\n    image: aic.sensors\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_VM_ID\n      - AIC_PLAYER_ENABLE_SENSORS\n      - AIC_PLAYER_ENABLE_BATTERY\n      - AIC_PLAYER_ENABLE_GPS\n      - AIC_PLAYER_ENABLE_GSM\n      - AIC_PLAYER_ENABLE_NFC\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  camera:\n    # command: tail -f /dev/null\n    container_name:  ${AIC_AVM_PREFIX}camera \n    restart: unless-stopped\n    image: aic.camera\n    environment:\n      - AIC_PLAYER_AMQP_HOST\n      - AIC_PLAYER_AMQP_USERNAME\n      - AIC_PLAYER_AMQP_PASSWORD\n      - AIC_PLAYER_VM_HOST\n      - AIC_PLAYER_VM_ID\n    volumes_from:\n      - container:${AIC_PROJECT_PREFIX}prjdata:ro\n    networks:\n      - default\n      - services_default\n    external_links:\n      - rabbitmq:rabbitmq\n  avmdata:\n    container_name:  ${AIC_AVM_PREFIX}avmdata \n    restart: unless-stopped\n    image: aic.avmdata\n    volumes:\n      - /data/avm\n    networks: []\n\nnetworks:\n  services_default:\n    external: true\n  default:\n\nvolumes:\n  nvidia_driver_370.28:\n    external: true", 
            "title": "docker-compose"
        }
    ]
}